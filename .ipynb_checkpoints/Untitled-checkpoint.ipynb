{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network - Long short-term memory (LSTM) \n",
    "\n",
    "The purpose of this study was to build a LSTM Model to Predict Stock Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # to aide in loading and manipulating our datasets.\n",
    "import numpy as np # for scientific computation\n",
    "import matplotlib.pyplot as plt # for plotting graphs\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1 - Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Dataset\n",
    "dataset_train = pd.read_csv('dataset_2012_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values\n",
    "dataset_train = dataset_train.dropna()\n",
    "# another way: dataset_train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1759, 7)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The shape property returns a tuple representing the dimensionality of the DataFrame. \n",
    "# The format of shape is (rows, columns)\n",
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>25.370001</td>\n",
       "      <td>26.340000</td>\n",
       "      <td>25.320000</td>\n",
       "      <td>26.110001</td>\n",
       "      <td>23.034264</td>\n",
       "      <td>12754300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>25.910000</td>\n",
       "      <td>26.580000</td>\n",
       "      <td>25.910000</td>\n",
       "      <td>26.459999</td>\n",
       "      <td>23.343037</td>\n",
       "      <td>12351500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>26.309999</td>\n",
       "      <td>26.370001</td>\n",
       "      <td>25.870001</td>\n",
       "      <td>26.110001</td>\n",
       "      <td>23.034264</td>\n",
       "      <td>8568600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-06</td>\n",
       "      <td>26.250000</td>\n",
       "      <td>26.250000</td>\n",
       "      <td>25.639999</td>\n",
       "      <td>25.690001</td>\n",
       "      <td>22.663742</td>\n",
       "      <td>8532100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-09</td>\n",
       "      <td>26.080000</td>\n",
       "      <td>26.969999</td>\n",
       "      <td>25.930000</td>\n",
       "      <td>26.879999</td>\n",
       "      <td>23.713554</td>\n",
       "      <td>26046600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Open       High        Low      Close  Adj Close    Volume\n",
       "0  2012-01-03  25.370001  26.340000  25.320000  26.110001  23.034264  12754300\n",
       "1  2012-01-04  25.910000  26.580000  25.910000  26.459999  23.343037  12351500\n",
       "2  2012-01-05  26.309999  26.370001  25.870001  26.110001  23.034264   8568600\n",
       "3  2012-01-06  26.250000  26.250000  25.639999  25.690001  22.663742   8532100\n",
       "4  2012-01-09  26.080000  26.969999  25.930000  26.879999  23.713554  26046600"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the head of our dataset to give us a glimpse into the kind of dataset we’re working with.\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the Open column that we’ll use in our modeling\n",
    "dataset_train = dataset_train.iloc[:, 1:2].values \n",
    "# notice ==>> df.iloc[:, 1:2] returns a dataframe whereas df.iloc[:, 1] returns a series \n",
    "# notice ==>\"Open\" column is the starting price while the Close column is the final price of a stock on a particular trading day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25.370001],\n",
       "       [25.91    ],\n",
       "       [26.309999],\n",
       "       ...,\n",
       "       [12.      ],\n",
       "       [12.56    ],\n",
       "       [13.01    ]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Scaling\n",
    " * We have to scale our data for optimal performance. \n",
    " * Scikit MinMaxScaler will scale our dataset to numbers between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of the training set - Transform features by scaling each feature to a given range.\n",
    "normalizer = MinMaxScaler(feature_range=(0,1))\n",
    "train_set_scaled = normalizer.fit_transform(dataset_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Data with Timesteps\n",
    "* LSTM expect our data to be in a specific format, usually a 3D array. \n",
    "* We start by creating data in 90 timesteps and converting it into an array using NumPy. \n",
    "* Next, we convert the data into a 3D dimension array with X_train samples, 90 timestamps, and one feature at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data with timesteps for the train_set_scaled\n",
    "# timesteps ==>> to train the algorithm, I will check 90 rows on each step  \n",
    "x_train = []\n",
    "y_real = []\n",
    "\n",
    "for i in range(45, 1759): \n",
    "    x_train.append(train_set_scaled[i-45:i, 0])  \n",
    "    # in other words => first iteration: [90-90:90, collumn index: 0] (from zero until 89)\n",
    "    # => segunda iteration: [91-90:91, collumn index: 0] (from 1 until 90)\n",
    "    # => segunda iteration: [92-90:92, collumn index: 0] (from 2 until 91)\n",
    "    # etc\n",
    "    y_real.append(train_set_scaled[i, 0])  # in other words => stores the index that it wants to predict \n",
    "     \n",
    " \n",
    "x_train, y_real = np.array(x_train), np.array(y_real) # I want the data to be in an numpy array\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1)) \n",
    "# Reshape method will create dimensions (it will have 3 dimensions: the two we had and one more)\n",
    "# NOTICE == >> we have to reshape our data to 3D because tensorflow requires it to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2 - Data Analysis - Building the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to build the LSTM, we need to import a couple of modules from Keras:\n",
    "\n",
    "from keras.models import Sequential #Sequential for initializing the neural network\n",
    "from keras.layers import Dense #Dense for adding a densely connected neural network layer\n",
    "from keras.layers import LSTM #LSTM for adding the Long Short-Term Memory layer\n",
    "from keras.layers import Dropout #Dropout for adding dropout layers that prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the LSTM\n",
    "* We add the LSTM layer and later add a few Dropout layers to prevent overfitting. \n",
    "* We add the LSTM layer with the following arguments:\n",
    "* 50 units which is the dimensionality of the output space\n",
    "* return_sequences=True (determines to return the last output in the output sequence, or the full sequence)\n",
    "* input_shape as the shape of our training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "54/54 [==============================] - 16s 60ms/step - loss: 0.0280 - mean_absolute_error: 0.1111\n",
      "Epoch 2/70\n",
      "54/54 [==============================] - 3s 62ms/step - loss: 0.0074 - mean_absolute_error: 0.0640\n",
      "Epoch 3/70\n",
      "54/54 [==============================] - 3s 60ms/step - loss: 0.0040 - mean_absolute_error: 0.0477\n",
      "Epoch 4/70\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 0.0049 - mean_absolute_error: 0.0515\n",
      "Epoch 5/70\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 0.0038 - mean_absolute_error: 0.0451\n",
      "Epoch 6/70\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 0.0032 - mean_absolute_error: 0.0419\n",
      "Epoch 7/70\n",
      "54/54 [==============================] - 3s 64ms/step - loss: 0.0029 - mean_absolute_error: 0.0400\n",
      "Epoch 8/70\n",
      "54/54 [==============================] - 3s 62ms/step - loss: 0.0029 - mean_absolute_error: 0.0396\n",
      "Epoch 9/70\n",
      "54/54 [==============================] - 3s 65ms/step - loss: 0.0025 - mean_absolute_error: 0.0371\n",
      "Epoch 10/70\n",
      "54/54 [==============================] - 4s 65ms/step - loss: 0.0025 - mean_absolute_error: 0.0369: 1s - loss: 0.0025 - \n",
      "Epoch 11/70\n",
      "54/54 [==============================] - 3s 63ms/step - loss: 0.0023 - mean_absolute_error: 0.0359\n",
      "Epoch 12/70\n",
      "54/54 [==============================] - 4s 65ms/step - loss: 0.0020 - mean_absolute_error: 0.0325: 0s - loss: 0.0020 - mean_absolute_e\n",
      "Epoch 13/70\n",
      "54/54 [==============================] - 4s 66ms/step - loss: 0.0017 - mean_absolute_error: 0.0313\n",
      "Epoch 14/70\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 0.0022 - mean_absolute_error: 0.0325\n",
      "Epoch 15/70\n",
      "54/54 [==============================] - 4s 69ms/step - loss: 0.0020 - mean_absolute_error: 0.0331\n",
      "Epoch 16/70\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 0.0019 - mean_absolute_error: 0.0326\n",
      "Epoch 17/70\n",
      "54/54 [==============================] - 4s 71ms/step - loss: 0.0016 - mean_absolute_error: 0.0295\n",
      "Epoch 18/70\n",
      "54/54 [==============================] - 4s 69ms/step - loss: 0.0016 - mean_absolute_error: 0.0293\n",
      "Epoch 19/70\n",
      "54/54 [==============================] - 4s 69ms/step - loss: 0.0016 - mean_absolute_error: 0.0286\n",
      "Epoch 20/70\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 0.0015 - mean_absolute_error: 0.0280: 1s - loss: 0.0016\n",
      "Epoch 21/70\n",
      "54/54 [==============================] - 4s 70ms/step - loss: 0.0016 - mean_absolute_error: 0.0289\n",
      "Epoch 22/70\n",
      "54/54 [==============================] - 4s 75ms/step - loss: 0.0014 - mean_absolute_error: 0.0281\n",
      "Epoch 23/70\n",
      "21/54 [==========>...................] - ETA: 2s - loss: 0.0016 - mean_absolute_error: 0.0292"
     ]
    }
   ],
   "source": [
    "regressor = Sequential() #  4 layers - each will have 50 neurons.  30% is the probability that each neuron will become inactivate during each epoch.\n",
    "#the result will be a bit worse, but in the testing phase i will have a better generalization\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (x_train.shape[1], 1)))\n",
    "regressor.add(Dropout(0.3))  \n",
    "\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.3))  \n",
    "\n",
    "regressor.add(LSTM(units = 50)) # here i don't need the history cos I only need the result\n",
    "regressor.add(Dropout(0.3))\n",
    "\n",
    "# we add the Dense layer that specifies the output of 1 unit, the output is 1 \n",
    "regressor.add(Dense(units = 1, activation = 'linear')) # last layer is a dense layer with linear because I have continuous values\n",
    "\n",
    "# we compile our model using the popular adam optimizer and set the loss as the mean_squarred_error. \n",
    "#This will compute the mean of the squared errors.  \n",
    "regressor.compile(optimizer = 'rmsprop', loss = 'mean_squared_error',metrics = ['mean_absolute_error'])\n",
    "# mean_squared_error = measures the average of the squares of the errors / \n",
    "# Notice - if one of the errors is too big, it will impact the results a lot. \n",
    "# This is why \"mean absolute percentage error\" is used as a great solution to calculate the error.\n",
    "regressor.fit(x_train, y_real, epochs = 70, batch_size = 32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Future Stock using the Test Set\n",
    "### In order to predict future stock prices, we will need to do a couple of steps: \n",
    "* Step 1) Load the test set, get rid of null values and choose the Open collumn \n",
    "* Step 2) Merge the training set and the test set on the 0 axis.\n",
    "* Step 3) Set the time step as 90 (as done previously) and reshape\n",
    "* Step 4) Use MinMaxScaler to transform and reshape the dataset as done previously\n",
    "* Step 5) After making the predictions we will have to use inverse_transform to get back the stock prices in normal readable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1) Load the test set, get rid of null values and choose the Open collumn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now loading the testing set\n",
    "dataset_test = pd.read_csv('dataset_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop null values\n",
    "dataset_test = dataset_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the Open column that we’ll use in our modeling\n",
    "real_stock_price = dataset_test.iloc[:, 1:2].values  \n",
    "# notice ==>> df.iloc[:, 1:2] returns a dataframe whereas df.iloc[:, 1] returns a series \n",
    "# notice ==>\"Open\" column is the starting price while the Close column is the final price of a stock on a particular trading day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test.Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Open':dataset_train[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2) Merge the training set and the test set on the 0 axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_totality = pd.concat((pd.DataFrame(dataset_train), dataset_test['Open']), axis = 0)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3) Set the time step as 90 (as done previously) reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dataset_totality[len(dataset_totality) - len(dataset_test) - 30:].values\n",
    "# No meu dataset total eu quero garantir q pego só as linhas do dataset treino.  \n",
    "inputs = inputs.reshape(-1, 1) #reshape to turn it into a vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4) Use MinMaxScaler to transform and reshape the dataset as done previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = normalizer.transform(inputs) # AFINAL = usar aqui so transform ou fit_transform \n",
    "# Normalization of the test set - Transform features by scaling each feature to a given range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Step 5) use inverse_transform to get back the stock prices in normal readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating data with timesteps for the train_set_scaled\n",
    "# timesteps ==>> checks 90 rows on each step on every sliding window\n",
    "x_test = []\n",
    "for i in range(45, 251):\n",
    "    x_test.append(inputs[i-45:i, 0]) # in other words => [90-90:90, collumn index: 0]\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    " \n",
    "# NOTICE == >> we have to reshape our data to 3D DataFrame (panel)\n",
    "# A data frame is a two-dimensional data structure, that is, the data is aligned in rows and columns in a table, a three-dimensional data structure is called panel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5) After making the predictions we will have to use inverse_transform to get back the stock prices in normal readable format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the predictions\n",
    "predicted_stock_price = regressor.predict(x_test)\n",
    "# Get back the stock prices in normal readable format\n",
    "predicted_stock_price = normalizer.inverse_transform(predicted_stock_price) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3 - Valuation of the Analysis - Plootting results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(real_stock_price, color = 'purple', label = 'Petrobras Stock Price')\n",
    "plt.plot(predicted_stock_price, color = 'green', label = 'Predicted Petrobras Stock Price')\n",
    "plt.title('Petrobras Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Petrobras Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTICE:\n",
    "# If I increase the range in the loop ==>> I can predict shorter periods of time ==>> but with MORE accuracy   \n",
    "# If I decrease the range in the loop ==>> I can predict longer pediods of time ==>> but with LESS accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
